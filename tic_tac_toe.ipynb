{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import numpy as np\n",
    "from numpy.random import normal, choice\n",
    "import pickle\n",
    "\n",
    "class TaskController:\n",
    "    def __init__(self, env, learning_algorithm='SARSA', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, online_learning=False,\n",
    "                 *args, **kwargs):\n",
    "        self.env = env\n",
    "        policy = Policy(self.env, states=self.env.all_states(), actions=self.env.all_actions(),\n",
    "                        state_action_validity_checker=self.env.is_state_action_pair_valid, \n",
    "                        algorithm=learning_algorithm, exploration=exploration, exploration_decay=exploration_decay, \n",
    "                        exploration_strategy=exploration_strategy, exploration_epsilon=exploration_epsilon, \n",
    "                        learning_rate=learning_rate, learning_rate_decay=learning_rate_decay, gamma=gamma, \n",
    "                        *args, **kwargs)\n",
    "        self.agent = Agent(policy, initial_state=self.env.get_initial_state())\n",
    "            \n",
    "        self.online_learning = online_learning\n",
    "        \n",
    "    def run_episode(self, learning_phase=True):\n",
    "        \n",
    "        action = self.agent.choose_action(self.agent.current_state, always_greedy=not learning_phase)\n",
    "        \n",
    "        self.agent.next_state, reward = self.env.get_next_state_reward(self.agent.current_state, action)\n",
    "        if learning_phase:\n",
    "            if self.online_learning:\n",
    "                next_action = None\n",
    "                if self.agent.policy.algorithm_type == 'SARSA':\n",
    "                    next_action = self.agent.choose_action(self.agent.next_state, always_greedy=True)\n",
    "                elif self.agent.policy.algorithm_type == 'QLEARNING':\n",
    "                    next_action = None\n",
    "                else:\n",
    "                    # EXPECTED_SARSA all actions are averaged\n",
    "                    next_action = None\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state, \n",
    "                                              action, next_action)\n",
    "            else:\n",
    "                self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                #print(self.agent.policy.history)\n",
    "        else:\n",
    "            self.agent.policy.history.append({'state': self.agent.current_state, 'action': action, 'reward': reward})\n",
    "                \n",
    "        if self.env.is_terminal(self.agent.next_state):\n",
    "            self.env.episode_terminated = True\n",
    "            if not self.online_learning and learning_phase:\n",
    "                self.agent.feed_reward(reward, self.agent.current_state, self.agent.next_state,\n",
    "                                                  None, None, online=False)\n",
    "                self.agent.reset_history()\n",
    "            \n",
    "        current_state = self.agent.current_state\n",
    "        self.agent.current_state = self.agent.next_state\n",
    "        \n",
    "        return current_state, action, self.agent.next_state, reward\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.episode_terminated = False\n",
    "        self.agent.current_state=self.env.get_initial_state()\n",
    "        \n",
    "        \n",
    "import numpy as np\n",
    "from numpy.random import random, choice\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, env, states={}, actions={}, state_action_validity_checker=None, \n",
    "                 hash_states=False, hash_actions=False,\n",
    "                 algorithm='QLEARNING', exploration=True, exploration_decay='SIMULATED_ANNEALING', \n",
    "                 exploration_strategy='E_GREEDY', exploration_epsilon=0.8, \n",
    "                 learning_rate=0.01, learning_rate_decay='EXPONENTIAL', gamma=0.9, *args, **kwargs):\n",
    "        # Set the update rule\n",
    "        self.env = env\n",
    "        self.algorithm_type = algorithm\n",
    "        \n",
    "        if algorithm=='QLEARNING':\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "        elif algorithm=='SARSA':\n",
    "            self.algorithm = Sarsa(learning_rate=learning_rate, \n",
    "                                   gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EXPECTED_SARSA':\n",
    "            self.algorithm = ExpectedSarsa(learning_rate=learning_rate, \n",
    "                                           gamma=gamma,*args, **kwargs)\n",
    "        elif algorithm=='EVERY_VISIT_MC':\n",
    "            self.algorithm = EveryVisitMC(*args, **kwargs)\n",
    "        elif algorithm=='FIRST_VISIT_MC':\n",
    "            self.algorithm = FirstVisitMC(*args, **kwargs)\n",
    "        else:\n",
    "            self.algorithm = QLearning(learning_rate=learning_rate,\n",
    "                                       gamma=gamma, *args, **kwargs)\n",
    "           \n",
    "        # Set exploration-exploitation strategy\n",
    "        self.exploration = exploration\n",
    "        if self.exploration:\n",
    "            self.exploration_decay = exploration_decay\n",
    "            self.exploration_strategy = exploration_strategy\n",
    "            self.exploration_epsilon = exploration_epsilon\n",
    "            \n",
    "        # Set LR and gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize policy\n",
    "        self.states = [hash(state) for state in states] if hash_states else states\n",
    "        \n",
    "        self.hash_states = hash_states\n",
    "        self.hash_actions = hash_actions\n",
    "        \n",
    "        if 'load_policy' not in kwargs:\n",
    "            self.policy = dict()\n",
    "            actions = list(actions)\n",
    "            for state in self.states:\n",
    "                temp = dict()\n",
    "                for action in actions:\n",
    "                    if state_action_validity_checker(state, action):\n",
    "                        temp[action] = 0.1\n",
    "                if len(temp.keys())>0:\n",
    "                    self.policy[state] = temp\n",
    "        else:\n",
    "            self.policy = self.load_policy(kwargs['load_policy'])\n",
    "        \n",
    "        self.history = []\n",
    "    \n",
    "    def feed_reward(self, reward, current_state, next_state, current_action, next_action, online=True):\n",
    "        if online:\n",
    "            self.algorithm.feed_reward(self, reward=reward, \n",
    "                                       current_state=current_state, \n",
    "                                       next_state=next_state, \n",
    "                                       current_action=current_action, \n",
    "                                       next_action=next_action)\n",
    "        else:\n",
    "            # Offline episodic updates\n",
    "            # History is an array of dicts [{'state': state, 'action': action},{...},...]\n",
    "            last_element = True\n",
    "            \n",
    "            for elm in reversed(self.history):\n",
    "                reward = elm['reward']\n",
    "                if last_element:\n",
    "                    last_element = False\n",
    "                    next_state = elm['state']\n",
    "                    next_action = elm['action']\n",
    "                current_state = elm['state']\n",
    "                current_action = elm['action']\n",
    "                self.algorithm.feed_reward(self, reward=reward, \n",
    "                                           current_state=current_state, \n",
    "                                           next_state=next_state, \n",
    "                                           current_action=current_action, \n",
    "                                           next_action=next_action)\n",
    "                next_state = current_state\n",
    "                next_action = current_action\n",
    "                \n",
    "            if self.algorithm_type in ['EVERY_VISIT_MC', 'FIRST_VISIT_MC']:\n",
    "                for state, temp in self.algorithm.visit_count.items():\n",
    "                    for action, value in temp.items():\n",
    "                        self.policy[state][action] = (self.policy[state][action]*(self.algorithm.num_iter) \n",
    "                                                      + np.mean(self.algorithm.visit_count[state][action]))/float(self.algorithm.num_iter+1)\n",
    "                self.algorithm.reset_episode()\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def choose_action(self, state, always_greedy=False):\n",
    "        action = None\n",
    "        if always_greedy:\n",
    "            return max(self.policy[state], key=self.policy[state].get)\n",
    "        if self.exploration:\n",
    "            # Learning Phase\n",
    "            if random() > self.exploration_epsilon:\n",
    "                # Exploitation\n",
    "                action = max(self.policy[state], key=self.policy[state].get)\n",
    "            else:\n",
    "                # Exploration\n",
    "                if self.exploration_strategy == 'E_GREEDY':\n",
    "                    min_v = min(self.policy[state].values())\n",
    "                    non_neg_v = [v - min_v + 0.1 for v in self.policy[state].values()]\n",
    "                    total = sum(non_neg_v)\n",
    "                    ordered_actions = [(k,float(v - min_v + 0.1)/total) for k,v in self.policy[state].items()]\n",
    "                    a = [a[0] for a in ordered_actions]\n",
    "                    p = [a[1] for a in ordered_actions]\n",
    "                    action = choice(a, p=p)\n",
    "                else:\n",
    "                    action = choice(list(self.policy[state].keys()))\n",
    "                \n",
    "                # Decay of exploration rate\n",
    "                if self.exploration_decay=='SIMULATED_ANNEALING':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon*0.99\n",
    "                elif self.exploration_decay=='CONSTANT':\n",
    "                    self.exploration_epsilon = self.exploration_epsilon\n",
    "        \n",
    "        else:\n",
    "            # Non Learning Phase\n",
    "            action = max(self.policy[state], key=self.policy[state].get)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def save_policy(self, filename='policy.pkl'):\n",
    "        with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "            pickle.dump(self.policy, output, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def load_policy(self, filename='policy.pkl'):\n",
    "        with open(filename, 'rb') as input:\n",
    "            policy = pickle.load(input)\n",
    "        return policy\n",
    "    \n",
    "    \n",
    "class Sarsa:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False, *args, **kwargs):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*policy.policy[next_state][next_action] - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class QLearning:\n",
    "    # Use >300 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False, *args, **kwargs):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*max(policy.policy[next_state].values()) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "class ExpectedSarsa:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, learning_rate=0.01, gamma=0.9, eligibility_trace=False, *args, **kwargs):\n",
    "        self.LR = learning_rate\n",
    "        self.GAMMA = gamma\n",
    "        self.num_iter = 0\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        policy.policy[current_state][current_action] = policy.policy[current_state][current_action] + self.LR*(\n",
    "            reward + self.GAMMA*np.mean(list(policy.policy[next_state].values())) - policy.policy[current_state][current_action])\n",
    "        \n",
    "        # Learning rate decay (logarithmic)\n",
    "        self.num_iter += 1\n",
    "        self.LR = min(1.0/np.log(self.num_iter), self.LR)\n",
    "        return self.num_iter\n",
    "    \n",
    "    \n",
    "class EveryVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9, *args, **kwargs):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            if current_action in self.visit_count[current_state]:\n",
    "                self.visit_count[current_state][current_action].append(self.episode_return)\n",
    "            else:\n",
    "                self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1\n",
    "\n",
    "    \n",
    "class FirstVisitMC:\n",
    "    # Use >1000 episodes\n",
    "    def __init__(self, gamma=0.9, *args, **kwargs):\n",
    "        self.num_iter = 1\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.GAMMA = gamma\n",
    "    \n",
    "    def feed_reward(self, policy, reward=None, current_state=None, next_state=None, current_action=None, next_action=None):\n",
    "        # Python is pass by name and mutability of object decides whether it acts as pass by value or reference\n",
    "        # Dicts are mutable and therefore this modifies the self.policy on Policy instance\n",
    "        self.episode_return += self.GAMMA*reward\n",
    "\n",
    "        if current_state in self.visit_count:\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "        else:\n",
    "            self.visit_count[current_state] = dict()\n",
    "            self.visit_count[current_state][current_action] = [self.episode_return]\n",
    "            \n",
    "        return self.num_iter\n",
    "    \n",
    "    def reset_episode(self):\n",
    "        self.visit_count = dict()\n",
    "        self.episode_return = 0\n",
    "        self.num_iter += 1\n",
    "        \n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, policy, initial_state=None):\n",
    "        self.current_state = initial_state\n",
    "        self.next_state = None\n",
    "        # Pass the generator object for both states and actions\n",
    "        self.policy = policy\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.policy.history = []\n",
    "        \n",
    "    def choose_action(self, *args, **kwargs):\n",
    "        return self.policy.choose_action(*args, **kwargs)\n",
    "    \n",
    "    def feed_reward(self, *args, **kwargs):\n",
    "        return self.policy.feed_reward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from operator import itemgetter\n",
    "class TicTacToe:\n",
    "    def __init__(self, sym='X'):\n",
    "        self.sym = sym\n",
    "        self.episode_terminated = False\n",
    "        \n",
    "    def all_states(self):\n",
    "        return product(\"_XO\",repeat=9)\n",
    "    \n",
    "    def all_actions(self):\n",
    "        return range(9)\n",
    "    \n",
    "    def get_next_state_reward(self, current_state, action):\n",
    "        state = list(current_state)\n",
    "        state[action] = self.sym\n",
    "        possible_actions = [i for i,sym in enumerate(state) if sym=='_']\n",
    "        state[choice(possible_actions)] = 'O' if self.sym=='X' else 'X'\n",
    "        return tuple(state), 1 if self.has_won(state, self.sym) else 0\n",
    "    \n",
    "    def is_state_action_pair_valid(self, state, action):\n",
    "        x_minus_o = 0\n",
    "        count_ = 0\n",
    "        for i in state:\n",
    "            if i=='X':\n",
    "                x_minus_o+=1 \n",
    "            elif i=='O':\n",
    "                x_minus_o-=1\n",
    "            else:\n",
    "                count_ +=1\n",
    "        if ((x_minus_o == 0 and self.sym=='X' and state[action]=='_') \n",
    "            or (x_minus_o == 1 and self.sym=='O' and state[action]=='_')):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        count_ = 0\n",
    "        for i in state:\n",
    "            if i=='_':\n",
    "                count_ = count_ + 1\n",
    "        return True if (self.has_won(state, 'X') or self.has_won(state, 'O') or count_==1) else False\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        return (\"_\",\"_\",\"_\",\"_\",\"_\",\"_\",\"_\",\"_\",\"_\")\n",
    "    \n",
    "    def __has_won(self, state, sym):\n",
    "        if state[0]==state[1]==state[2]==self.sym:\n",
    "            return True\n",
    "        elif state[3]==state[4]==state[5]==self.sym:\n",
    "            return True\n",
    "        elif state[6]==state[7]==state[8]==self.sym:\n",
    "            return True\n",
    "        elif state[0]==state[4]==state[8]==self.sym:\n",
    "            return True\n",
    "        elif state[2]==state[4]==state[6]==self.sym:\n",
    "            return True\n",
    "        elif state[0]==state[3]==state[6]==self.sym:\n",
    "            return True\n",
    "        elif state[1]==state[4]==state[7]==self.sym:\n",
    "            return True\n",
    "        elif state[2]==state[5]==state[8]==self.sym:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def has_won(self, state, sym):\n",
    "        count_ = 0\n",
    "        for i in state:\n",
    "            if i=='_':\n",
    "                count_ = count_ + 1\n",
    "        if count_==1:\n",
    "            last_state = [i if i != '_' else 'X' for i in state]\n",
    "            return self.__has_won(state, sym) or self.has_won(last_state, sym)\n",
    "        else:\n",
    "            return self.__has_won(state, sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nOPTIONAL PARAMETERS - \\nload_policy='FILE_NAME'  eg: load_policy='policy.pkl'\\neligibility_trace = True (NOT IMPLEMENTED HERE)\\n\\nYOU CAN SAVE A POLICY BY CALLING - \\ntc.agent.policy.save_policy('FILE_NAME')  eg: tc.agent.policy.save_policy('policy.pkl')\\n\\nCASE STUDY 1 - YOU WANT TO INCREMENTALLY ADD STATES, FOR EXAMPLE\\nFIRST TRAIN USING JUST RED AND GREEN SIGNALS, SAVE THE RESULTING POLICY\\nRECREATE tc WITH load_policy OPTION AND SAVED POLICY AND TRAIN\\nBUT NOW WITH ADDITIONAL STATE OF YELLOW SIGNAL\\n\\nIN THIS CASE, THE INITIAL POLICY *SHOULD* ALSO HAVE RED, GREEN AND YELLOW SIGNALS, JUST THAT\\nYELLOW IS NEVER ENCOUNTERED (SWITCHING TIME IS 0). IN THE SECOND ROUND, YOU CAN INTRODUCE YELLOW SIGNAL. FOR EXAMPLE BY\\nMAKING SWITCHING TIME OF YELLOW SIGNAL NON 0.\\n\\nCASE STUDY 2 - TRAIN USING SARSA FIRST, SAVE POLICY, CREATE NEW TC WITH SAVED POLICY AND QLEARNING, TRAIN FROM THERE. \\nMAKE SURE TO CREATE NEW TC WITH MANUALLY DECAYED LEARNING/EXPLORATION RATE \\n\\nELIGIBILITY TRACE IS IMPORTANT IN THIS TASK TO LEARN EFFICIENTLY. BUT IT IS NOT IMPLEMENTED IN THIS VERSION\\nIT CAN BE TURNED ON/OFF USING OPTIONAL eligibility_trace PARAMETER\\n\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = TicTacToe(sym='X')\n",
    "\n",
    "tc = TaskController(p1, \n",
    "                    learning_algorithm='QLEARNING', # QLEARNING, SARSA, EXPECTED_SARSA, FIRST_VISIT_MC, EVERY_VISIT_MC\n",
    "                    exploration=True, \n",
    "                    exploration_decay='CONSTANT', # CONSTANT, SIMULATED_ANNEALING\n",
    "                    exploration_strategy='E_GREEDY', # E_GREEDY, SOFT_E_GREEDY\n",
    "                    exploration_epsilon=0.99, \n",
    "                    learning_rate=0.1, \n",
    "                    learning_rate_decay='EXPONENTIAL', # CONSTANT, EXPONENTIAL\n",
    "                    gamma=0.9,\n",
    "                    online_learning=True,\n",
    "                    load_policy='tttpolicy.pkl'\n",
    "                   )\n",
    "\n",
    "\n",
    "'''\n",
    "OPTIONAL PARAMETERS - \n",
    "load_policy='FILE_NAME'  eg: load_policy='policy.pkl'\n",
    "eligibility_trace = True (NOT IMPLEMENTED HERE)\n",
    "\n",
    "YOU CAN SAVE A POLICY BY CALLING - \n",
    "tc.agent.policy.save_policy('FILE_NAME')  eg: tc.agent.policy.save_policy('policy.pkl')\n",
    "\n",
    "CASE STUDY 1 - YOU WANT TO INCREMENTALLY ADD STATES, FOR EXAMPLE\n",
    "FIRST TRAIN USING JUST RED AND GREEN SIGNALS, SAVE THE RESULTING POLICY\n",
    "RECREATE tc WITH load_policy OPTION AND SAVED POLICY AND TRAIN\n",
    "BUT NOW WITH ADDITIONAL STATE OF YELLOW SIGNAL\n",
    "\n",
    "IN THIS CASE, THE INITIAL POLICY *SHOULD* ALSO HAVE RED, GREEN AND YELLOW SIGNALS, JUST THAT\n",
    "YELLOW IS NEVER ENCOUNTERED (SWITCHING TIME IS 0). IN THE SECOND ROUND, YOU CAN INTRODUCE YELLOW SIGNAL. FOR EXAMPLE BY\n",
    "MAKING SWITCHING TIME OF YELLOW SIGNAL NON 0.\n",
    "\n",
    "CASE STUDY 2 - TRAIN USING SARSA FIRST, SAVE POLICY, CREATE NEW TC WITH SAVED POLICY AND QLEARNING, TRAIN FROM THERE. \n",
    "MAKE SURE TO CREATE NEW TC WITH MANUALLY DECAYED LEARNING/EXPLORATION RATE \n",
    "\n",
    "ELIGIBILITY TRACE IS IMPORTANT IN THIS TASK TO LEARN EFFICIENTLY. BUT IT IS NOT IMPLEMENTED IN THIS VERSION\n",
    "IT CAN BE TURNED ON/OFF USING OPTIONAL eligibility_trace PARAMETER\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaurav/Personal/Projects/cenv/lib/python3.6/site-packages/ipykernel_launcher.py:240: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 earned after 0 episodes P1\n",
      "1 earned after 100 episodes P1\n",
      "1 earned after 200 episodes P1\n",
      "1 earned after 300 episodes P1\n",
      "0 earned after 400 episodes P1\n",
      "1 earned after 500 episodes P1\n",
      "1 earned after 600 episodes P1\n",
      "1 earned after 700 episodes P1\n",
      "1 earned after 800 episodes P1\n",
      "1 earned after 900 episodes P1\n",
      "0 earned after 1000 episodes P1\n",
      "1 earned after 1100 episodes P1\n",
      "1 earned after 1200 episodes P1\n",
      "1 earned after 1300 episodes P1\n",
      "1 earned after 1400 episodes P1\n",
      "1 earned after 1500 episodes P1\n",
      "1 earned after 1600 episodes P1\n",
      "1 earned after 1700 episodes P1\n",
      "1 earned after 1800 episodes P1\n",
      "0 earned after 1900 episodes P1\n",
      "1 earned after 2000 episodes P1\n",
      "1 earned after 2100 episodes P1\n",
      "1 earned after 2200 episodes P1\n",
      "1 earned after 2300 episodes P1\n",
      "1 earned after 2400 episodes P1\n",
      "0 earned after 2500 episodes P1\n",
      "1 earned after 2600 episodes P1\n",
      "1 earned after 2700 episodes P1\n",
      "1 earned after 2800 episodes P1\n",
      "1 earned after 2900 episodes P1\n",
      "1 earned after 3000 episodes P1\n",
      "0 earned after 3100 episodes P1\n",
      "1 earned after 3200 episodes P1\n",
      "1 earned after 3300 episodes P1\n",
      "1 earned after 3400 episodes P1\n",
      "1 earned after 3500 episodes P1\n",
      "1 earned after 3600 episodes P1\n",
      "1 earned after 3700 episodes P1\n",
      "1 earned after 3800 episodes P1\n",
      "1 earned after 3900 episodes P1\n",
      "1 earned after 4000 episodes P1\n",
      "0 earned after 4100 episodes P1\n",
      "0 earned after 4200 episodes P1\n",
      "1 earned after 4300 episodes P1\n",
      "1 earned after 4400 episodes P1\n",
      "1 earned after 4500 episodes P1\n",
      "1 earned after 4600 episodes P1\n",
      "1 earned after 4700 episodes P1\n",
      "1 earned after 4800 episodes P1\n",
      "0 earned after 4900 episodes P1\n",
      "1 earned after 5000 episodes P1\n",
      "1 earned after 5100 episodes P1\n",
      "1 earned after 5200 episodes P1\n",
      "0 earned after 5300 episodes P1\n",
      "1 earned after 5400 episodes P1\n",
      "1 earned after 5500 episodes P1\n",
      "1 earned after 5600 episodes P1\n",
      "1 earned after 5700 episodes P1\n",
      "1 earned after 5800 episodes P1\n",
      "1 earned after 5900 episodes P1\n",
      "0 earned after 6000 episodes P1\n",
      "1 earned after 6100 episodes P1\n",
      "1 earned after 6200 episodes P1\n",
      "1 earned after 6300 episodes P1\n",
      "1 earned after 6400 episodes P1\n",
      "1 earned after 6500 episodes P1\n",
      "1 earned after 6600 episodes P1\n",
      "1 earned after 6700 episodes P1\n",
      "0 earned after 6800 episodes P1\n",
      "0 earned after 6900 episodes P1\n",
      "1 earned after 7000 episodes P1\n",
      "1 earned after 7100 episodes P1\n",
      "1 earned after 7200 episodes P1\n",
      "0 earned after 7300 episodes P1\n",
      "1 earned after 7400 episodes P1\n",
      "1 earned after 7500 episodes P1\n",
      "1 earned after 7600 episodes P1\n",
      "0 earned after 7700 episodes P1\n",
      "1 earned after 7800 episodes P1\n",
      "1 earned after 7900 episodes P1\n",
      "1 earned after 8000 episodes P1\n",
      "1 earned after 8100 episodes P1\n",
      "0 earned after 8200 episodes P1\n",
      "1 earned after 8300 episodes P1\n",
      "1 earned after 8400 episodes P1\n",
      "0 earned after 8500 episodes P1\n",
      "1 earned after 8600 episodes P1\n",
      "1 earned after 8700 episodes P1\n",
      "1 earned after 8800 episodes P1\n",
      "1 earned after 8900 episodes P1\n",
      "1 earned after 9000 episodes P1\n",
      "1 earned after 9100 episodes P1\n",
      "1 earned after 9200 episodes P1\n",
      "1 earned after 9300 episodes P1\n",
      "1 earned after 9400 episodes P1\n",
      "1 earned after 9500 episodes P1\n",
      "1 earned after 9600 episodes P1\n",
      "1 earned after 9700 episodes P1\n",
      "0 earned after 9800 episodes P1\n",
      "1 earned after 9900 episodes P1\n",
      "1 earned after 10000 episodes P1\n",
      "1 earned after 10100 episodes P1\n",
      "1 earned after 10200 episodes P1\n",
      "1 earned after 10300 episodes P1\n",
      "1 earned after 10400 episodes P1\n",
      "1 earned after 10500 episodes P1\n",
      "1 earned after 10600 episodes P1\n",
      "1 earned after 10700 episodes P1\n",
      "1 earned after 10800 episodes P1\n",
      "1 earned after 10900 episodes P1\n",
      "1 earned after 11000 episodes P1\n",
      "1 earned after 11100 episodes P1\n",
      "1 earned after 11200 episodes P1\n",
      "1 earned after 11300 episodes P1\n",
      "0 earned after 11400 episodes P1\n",
      "0 earned after 11500 episodes P1\n",
      "1 earned after 11600 episodes P1\n",
      "1 earned after 11700 episodes P1\n",
      "1 earned after 11800 episodes P1\n",
      "1 earned after 11900 episodes P1\n",
      "1 earned after 12000 episodes P1\n",
      "1 earned after 12100 episodes P1\n",
      "1 earned after 12200 episodes P1\n",
      "1 earned after 12300 episodes P1\n",
      "1 earned after 12400 episodes P1\n",
      "1 earned after 12500 episodes P1\n",
      "0 earned after 12600 episodes P1\n",
      "1 earned after 12700 episodes P1\n",
      "1 earned after 12800 episodes P1\n",
      "1 earned after 12900 episodes P1\n",
      "1 earned after 13000 episodes P1\n",
      "1 earned after 13100 episodes P1\n",
      "1 earned after 13200 episodes P1\n",
      "1 earned after 13300 episodes P1\n",
      "1 earned after 13400 episodes P1\n",
      "1 earned after 13500 episodes P1\n",
      "1 earned after 13600 episodes P1\n",
      "1 earned after 13700 episodes P1\n",
      "1 earned after 13800 episodes P1\n",
      "1 earned after 13900 episodes P1\n",
      "1 earned after 14000 episodes P1\n",
      "1 earned after 14100 episodes P1\n",
      "1 earned after 14200 episodes P1\n",
      "1 earned after 14300 episodes P1\n",
      "1 earned after 14400 episodes P1\n",
      "1 earned after 14500 episodes P1\n",
      "1 earned after 14600 episodes P1\n",
      "1 earned after 14700 episodes P1\n",
      "1 earned after 14800 episodes P1\n",
      "1 earned after 14900 episodes P1\n",
      "1 earned after 15000 episodes P1\n",
      "1 earned after 15100 episodes P1\n",
      "1 earned after 15200 episodes P1\n",
      "1 earned after 15300 episodes P1\n",
      "1 earned after 15400 episodes P1\n",
      "1 earned after 15500 episodes P1\n",
      "1 earned after 15600 episodes P1\n",
      "1 earned after 15700 episodes P1\n",
      "1 earned after 15800 episodes P1\n",
      "1 earned after 15900 episodes P1\n",
      "1 earned after 16000 episodes P1\n",
      "0 earned after 16100 episodes P1\n",
      "1 earned after 16200 episodes P1\n",
      "1 earned after 16300 episodes P1\n",
      "0 earned after 16400 episodes P1\n",
      "1 earned after 16500 episodes P1\n",
      "1 earned after 16600 episodes P1\n",
      "1 earned after 16700 episodes P1\n",
      "1 earned after 16800 episodes P1\n",
      "1 earned after 16900 episodes P1\n",
      "1 earned after 17000 episodes P1\n",
      "1 earned after 17100 episodes P1\n",
      "1 earned after 17200 episodes P1\n",
      "1 earned after 17300 episodes P1\n",
      "1 earned after 17400 episodes P1\n",
      "1 earned after 17500 episodes P1\n",
      "1 earned after 17600 episodes P1\n",
      "1 earned after 17700 episodes P1\n",
      "1 earned after 17800 episodes P1\n",
      "1 earned after 17900 episodes P1\n",
      "1 earned after 18000 episodes P1\n",
      "0 earned after 18100 episodes P1\n",
      "1 earned after 18200 episodes P1\n",
      "1 earned after 18300 episodes P1\n",
      "1 earned after 18400 episodes P1\n",
      "1 earned after 18500 episodes P1\n",
      "1 earned after 18600 episodes P1\n",
      "1 earned after 18700 episodes P1\n",
      "1 earned after 18800 episodes P1\n",
      "1 earned after 18900 episodes P1\n",
      "1 earned after 19000 episodes P1\n",
      "1 earned after 19100 episodes P1\n",
      "1 earned after 19200 episodes P1\n",
      "1 earned after 19300 episodes P1\n",
      "1 earned after 19400 episodes P1\n",
      "1 earned after 19500 episodes P1\n",
      "1 earned after 19600 episodes P1\n",
      "1 earned after 19700 episodes P1\n",
      "1 earned after 19800 episodes P1\n",
      "1 earned after 19900 episodes P1\n",
      "1 earned after 20000 episodes P1\n",
      "1 earned after 20100 episodes P1\n",
      "1 earned after 20200 episodes P1\n",
      "1 earned after 20300 episodes P1\n",
      "1 earned after 20400 episodes P1\n",
      "1 earned after 20500 episodes P1\n",
      "0 earned after 20600 episodes P1\n",
      "1 earned after 20700 episodes P1\n",
      "1 earned after 20800 episodes P1\n",
      "1 earned after 20900 episodes P1\n",
      "1 earned after 21000 episodes P1\n",
      "1 earned after 21100 episodes P1\n",
      "0 earned after 21200 episodes P1\n",
      "1 earned after 21300 episodes P1\n",
      "1 earned after 21400 episodes P1\n",
      "1 earned after 21500 episodes P1\n",
      "1 earned after 21600 episodes P1\n",
      "1 earned after 21700 episodes P1\n",
      "1 earned after 21800 episodes P1\n",
      "0 earned after 21900 episodes P1\n",
      "1 earned after 22000 episodes P1\n",
      "1 earned after 22100 episodes P1\n",
      "1 earned after 22200 episodes P1\n",
      "1 earned after 22300 episodes P1\n",
      "1 earned after 22400 episodes P1\n",
      "1 earned after 22500 episodes P1\n",
      "1 earned after 22600 episodes P1\n",
      "1 earned after 22700 episodes P1\n",
      "1 earned after 22800 episodes P1\n",
      "1 earned after 22900 episodes P1\n",
      "0 earned after 23000 episodes P1\n",
      "1 earned after 23100 episodes P1\n",
      "1 earned after 23200 episodes P1\n",
      "1 earned after 23300 episodes P1\n",
      "1 earned after 23400 episodes P1\n",
      "1 earned after 23500 episodes P1\n",
      "1 earned after 23600 episodes P1\n",
      "1 earned after 23700 episodes P1\n",
      "1 earned after 23800 episodes P1\n",
      "1 earned after 23900 episodes P1\n",
      "1 earned after 24000 episodes P1\n",
      "1 earned after 24100 episodes P1\n",
      "1 earned after 24200 episodes P1\n",
      "1 earned after 24300 episodes P1\n",
      "1 earned after 24400 episodes P1\n",
      "0 earned after 24500 episodes P1\n",
      "1 earned after 24600 episodes P1\n",
      "1 earned after 24700 episodes P1\n",
      "1 earned after 24800 episodes P1\n",
      "1 earned after 24900 episodes P1\n",
      "1 earned after 25000 episodes P1\n",
      "1 earned after 25100 episodes P1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 earned after 25200 episodes P1\n",
      "1 earned after 25300 episodes P1\n",
      "1 earned after 25400 episodes P1\n",
      "1 earned after 25500 episodes P1\n",
      "1 earned after 25600 episodes P1\n",
      "1 earned after 25700 episodes P1\n",
      "1 earned after 25800 episodes P1\n",
      "1 earned after 25900 episodes P1\n",
      "1 earned after 26000 episodes P1\n",
      "1 earned after 26100 episodes P1\n",
      "0 earned after 26200 episodes P1\n",
      "1 earned after 26300 episodes P1\n",
      "1 earned after 26400 episodes P1\n",
      "1 earned after 26500 episodes P1\n",
      "1 earned after 26600 episodes P1\n",
      "1 earned after 26700 episodes P1\n",
      "1 earned after 26800 episodes P1\n",
      "1 earned after 26900 episodes P1\n",
      "1 earned after 27000 episodes P1\n",
      "1 earned after 27100 episodes P1\n",
      "1 earned after 27200 episodes P1\n",
      "1 earned after 27300 episodes P1\n",
      "1 earned after 27400 episodes P1\n",
      "1 earned after 27500 episodes P1\n",
      "1 earned after 27600 episodes P1\n",
      "1 earned after 27700 episodes P1\n",
      "1 earned after 27800 episodes P1\n",
      "1 earned after 27900 episodes P1\n",
      "1 earned after 28000 episodes P1\n",
      "1 earned after 28100 episodes P1\n",
      "1 earned after 28200 episodes P1\n",
      "1 earned after 28300 episodes P1\n",
      "1 earned after 28400 episodes P1\n",
      "1 earned after 28500 episodes P1\n",
      "1 earned after 28600 episodes P1\n",
      "1 earned after 28700 episodes P1\n",
      "1 earned after 28800 episodes P1\n",
      "1 earned after 28900 episodes P1\n",
      "1 earned after 29000 episodes P1\n",
      "1 earned after 29100 episodes P1\n",
      "0 earned after 29200 episodes P1\n",
      "0 earned after 29300 episodes P1\n",
      "1 earned after 29400 episodes P1\n",
      "0 earned after 29500 episodes P1\n",
      "1 earned after 29600 episodes P1\n",
      "1 earned after 29700 episodes P1\n",
      "1 earned after 29800 episodes P1\n",
      "1 earned after 29900 episodes P1\n",
      "1 earned after 30000 episodes P1\n",
      "0 earned after 30100 episodes P1\n",
      "1 earned after 30200 episodes P1\n",
      "1 earned after 30300 episodes P1\n",
      "1 earned after 30400 episodes P1\n",
      "1 earned after 30500 episodes P1\n",
      "1 earned after 30600 episodes P1\n",
      "1 earned after 30700 episodes P1\n",
      "1 earned after 30800 episodes P1\n",
      "0 earned after 30900 episodes P1\n",
      "1 earned after 31000 episodes P1\n",
      "1 earned after 31100 episodes P1\n",
      "1 earned after 31200 episodes P1\n",
      "0 earned after 31300 episodes P1\n",
      "1 earned after 31400 episodes P1\n",
      "1 earned after 31500 episodes P1\n",
      "1 earned after 31600 episodes P1\n",
      "1 earned after 31700 episodes P1\n",
      "1 earned after 31800 episodes P1\n",
      "1 earned after 31900 episodes P1\n",
      "1 earned after 32000 episodes P1\n",
      "1 earned after 32100 episodes P1\n",
      "1 earned after 32200 episodes P1\n",
      "1 earned after 32300 episodes P1\n",
      "0 earned after 32400 episodes P1\n",
      "1 earned after 32500 episodes P1\n",
      "1 earned after 32600 episodes P1\n",
      "1 earned after 32700 episodes P1\n",
      "1 earned after 32800 episodes P1\n",
      "1 earned after 32900 episodes P1\n",
      "1 earned after 33000 episodes P1\n",
      "1 earned after 33100 episodes P1\n",
      "1 earned after 33200 episodes P1\n",
      "1 earned after 33300 episodes P1\n",
      "1 earned after 33400 episodes P1\n",
      "1 earned after 33500 episodes P1\n",
      "1 earned after 33600 episodes P1\n",
      "0 earned after 33700 episodes P1\n",
      "1 earned after 33800 episodes P1\n",
      "1 earned after 33900 episodes P1\n",
      "1 earned after 34000 episodes P1\n",
      "1 earned after 34100 episodes P1\n",
      "1 earned after 34200 episodes P1\n",
      "0 earned after 34300 episodes P1\n",
      "1 earned after 34400 episodes P1\n",
      "1 earned after 34500 episodes P1\n",
      "1 earned after 34600 episodes P1\n",
      "1 earned after 34700 episodes P1\n",
      "1 earned after 34800 episodes P1\n",
      "1 earned after 34900 episodes P1\n",
      "1 earned after 35000 episodes P1\n",
      "0 earned after 35100 episodes P1\n",
      "0 earned after 35200 episodes P1\n",
      "1 earned after 35300 episodes P1\n",
      "1 earned after 35400 episodes P1\n",
      "1 earned after 35500 episodes P1\n",
      "1 earned after 35600 episodes P1\n",
      "1 earned after 35700 episodes P1\n",
      "0 earned after 35800 episodes P1\n",
      "1 earned after 35900 episodes P1\n",
      "1 earned after 36000 episodes P1\n",
      "1 earned after 36100 episodes P1\n",
      "0 earned after 36200 episodes P1\n",
      "1 earned after 36300 episodes P1\n",
      "1 earned after 36400 episodes P1\n",
      "1 earned after 36500 episodes P1\n",
      "1 earned after 36600 episodes P1\n",
      "1 earned after 36700 episodes P1\n",
      "1 earned after 36800 episodes P1\n",
      "1 earned after 36900 episodes P1\n",
      "1 earned after 37000 episodes P1\n",
      "1 earned after 37100 episodes P1\n",
      "0 earned after 37200 episodes P1\n",
      "1 earned after 37300 episodes P1\n",
      "1 earned after 37400 episodes P1\n",
      "1 earned after 37500 episodes P1\n",
      "1 earned after 37600 episodes P1\n",
      "1 earned after 37700 episodes P1\n",
      "1 earned after 37800 episodes P1\n",
      "1 earned after 37900 episodes P1\n",
      "0 earned after 38000 episodes P1\n",
      "1 earned after 38100 episodes P1\n",
      "1 earned after 38200 episodes P1\n",
      "1 earned after 38300 episodes P1\n",
      "1 earned after 38400 episodes P1\n",
      "1 earned after 38500 episodes P1\n",
      "1 earned after 38600 episodes P1\n",
      "1 earned after 38700 episodes P1\n",
      "1 earned after 38800 episodes P1\n",
      "1 earned after 38900 episodes P1\n",
      "0 earned after 39000 episodes P1\n",
      "1 earned after 39100 episodes P1\n",
      "1 earned after 39200 episodes P1\n",
      "0 earned after 39300 episodes P1\n",
      "1 earned after 39400 episodes P1\n",
      "1 earned after 39500 episodes P1\n",
      "0 earned after 39600 episodes P1\n",
      "1 earned after 39700 episodes P1\n",
      "1 earned after 39800 episodes P1\n",
      "1 earned after 39900 episodes P1\n",
      "1 earned after 40000 episodes P1\n",
      "1 earned after 40100 episodes P1\n",
      "0 earned after 40200 episodes P1\n",
      "1 earned after 40300 episodes P1\n",
      "1 earned after 40400 episodes P1\n",
      "1 earned after 40500 episodes P1\n",
      "1 earned after 40600 episodes P1\n",
      "1 earned after 40700 episodes P1\n",
      "1 earned after 40800 episodes P1\n",
      "1 earned after 40900 episodes P1\n",
      "1 earned after 41000 episodes P1\n",
      "1 earned after 41100 episodes P1\n",
      "1 earned after 41200 episodes P1\n",
      "1 earned after 41300 episodes P1\n",
      "1 earned after 41400 episodes P1\n",
      "1 earned after 41500 episodes P1\n",
      "1 earned after 41600 episodes P1\n",
      "1 earned after 41700 episodes P1\n",
      "1 earned after 41800 episodes P1\n",
      "1 earned after 41900 episodes P1\n",
      "1 earned after 42000 episodes P1\n",
      "1 earned after 42100 episodes P1\n",
      "1 earned after 42200 episodes P1\n",
      "1 earned after 42300 episodes P1\n",
      "1 earned after 42400 episodes P1\n",
      "1 earned after 42500 episodes P1\n",
      "1 earned after 42600 episodes P1\n",
      "1 earned after 42700 episodes P1\n",
      "0 earned after 42800 episodes P1\n",
      "1 earned after 42900 episodes P1\n",
      "1 earned after 43000 episodes P1\n",
      "1 earned after 43100 episodes P1\n",
      "1 earned after 43200 episodes P1\n",
      "1 earned after 43300 episodes P1\n",
      "1 earned after 43400 episodes P1\n",
      "0 earned after 43500 episodes P1\n",
      "0 earned after 43600 episodes P1\n",
      "1 earned after 43700 episodes P1\n",
      "1 earned after 43800 episodes P1\n",
      "1 earned after 43900 episodes P1\n",
      "1 earned after 44000 episodes P1\n",
      "1 earned after 44100 episodes P1\n",
      "1 earned after 44200 episodes P1\n",
      "1 earned after 44300 episodes P1\n",
      "1 earned after 44400 episodes P1\n",
      "1 earned after 44500 episodes P1\n",
      "1 earned after 44600 episodes P1\n",
      "1 earned after 44700 episodes P1\n",
      "1 earned after 44800 episodes P1\n",
      "1 earned after 44900 episodes P1\n",
      "1 earned after 45000 episodes P1\n",
      "1 earned after 45100 episodes P1\n",
      "1 earned after 45200 episodes P1\n",
      "1 earned after 45300 episodes P1\n",
      "0 earned after 45400 episodes P1\n",
      "1 earned after 45500 episodes P1\n",
      "1 earned after 45600 episodes P1\n",
      "1 earned after 45700 episodes P1\n",
      "1 earned after 45800 episodes P1\n",
      "1 earned after 45900 episodes P1\n",
      "1 earned after 46000 episodes P1\n",
      "1 earned after 46100 episodes P1\n",
      "1 earned after 46200 episodes P1\n",
      "1 earned after 46300 episodes P1\n",
      "1 earned after 46400 episodes P1\n",
      "1 earned after 46500 episodes P1\n",
      "1 earned after 46600 episodes P1\n",
      "1 earned after 46700 episodes P1\n",
      "0 earned after 46800 episodes P1\n",
      "0 earned after 46900 episodes P1\n",
      "1 earned after 47000 episodes P1\n",
      "1 earned after 47100 episodes P1\n",
      "1 earned after 47200 episodes P1\n",
      "1 earned after 47300 episodes P1\n",
      "1 earned after 47400 episodes P1\n",
      "1 earned after 47500 episodes P1\n",
      "1 earned after 47600 episodes P1\n",
      "1 earned after 47700 episodes P1\n",
      "0 earned after 47800 episodes P1\n",
      "1 earned after 47900 episodes P1\n",
      "1 earned after 48000 episodes P1\n",
      "1 earned after 48100 episodes P1\n",
      "1 earned after 48200 episodes P1\n",
      "1 earned after 48300 episodes P1\n",
      "1 earned after 48400 episodes P1\n",
      "1 earned after 48500 episodes P1\n",
      "0 earned after 48600 episodes P1\n",
      "1 earned after 48700 episodes P1\n",
      "1 earned after 48800 episodes P1\n",
      "1 earned after 48900 episodes P1\n",
      "1 earned after 49000 episodes P1\n",
      "1 earned after 49100 episodes P1\n",
      "0 earned after 49200 episodes P1\n",
      "1 earned after 49300 episodes P1\n",
      "1 earned after 49400 episodes P1\n",
      "1 earned after 49500 episodes P1\n",
      "1 earned after 49600 episodes P1\n",
      "0 earned after 49700 episodes P1\n",
      "0 earned after 49800 episodes P1\n",
      "1 earned after 49900 episodes P1\n",
      "1 earned after 50000 episodes P1\n",
      "1 earned after 50100 episodes P1\n",
      "1 earned after 50200 episodes P1\n",
      "1 earned after 50300 episodes P1\n",
      "0 earned after 50400 episodes P1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 earned after 50500 episodes P1\n",
      "0 earned after 50600 episodes P1\n",
      "0 earned after 50700 episodes P1\n",
      "1 earned after 50800 episodes P1\n",
      "1 earned after 50900 episodes P1\n",
      "1 earned after 51000 episodes P1\n",
      "1 earned after 51100 episodes P1\n",
      "1 earned after 51200 episodes P1\n",
      "1 earned after 51300 episodes P1\n",
      "1 earned after 51400 episodes P1\n",
      "1 earned after 51500 episodes P1\n",
      "1 earned after 51600 episodes P1\n",
      "1 earned after 51700 episodes P1\n",
      "1 earned after 51800 episodes P1\n",
      "1 earned after 51900 episodes P1\n",
      "1 earned after 52000 episodes P1\n",
      "1 earned after 52100 episodes P1\n",
      "0 earned after 52200 episodes P1\n",
      "0 earned after 52300 episodes P1\n",
      "1 earned after 52400 episodes P1\n",
      "0 earned after 52500 episodes P1\n",
      "1 earned after 52600 episodes P1\n",
      "1 earned after 52700 episodes P1\n",
      "1 earned after 52800 episodes P1\n",
      "1 earned after 52900 episodes P1\n",
      "1 earned after 53000 episodes P1\n",
      "1 earned after 53100 episodes P1\n",
      "1 earned after 53200 episodes P1\n",
      "1 earned after 53300 episodes P1\n",
      "1 earned after 53400 episodes P1\n",
      "1 earned after 53500 episodes P1\n",
      "1 earned after 53600 episodes P1\n",
      "1 earned after 53700 episodes P1\n",
      "1 earned after 53800 episodes P1\n",
      "1 earned after 53900 episodes P1\n",
      "1 earned after 54000 episodes P1\n",
      "1 earned after 54100 episodes P1\n",
      "1 earned after 54200 episodes P1\n",
      "1 earned after 54300 episodes P1\n",
      "1 earned after 54400 episodes P1\n",
      "1 earned after 54500 episodes P1\n",
      "1 earned after 54600 episodes P1\n",
      "1 earned after 54700 episodes P1\n",
      "1 earned after 54800 episodes P1\n",
      "1 earned after 54900 episodes P1\n",
      "1 earned after 55000 episodes P1\n",
      "1 earned after 55100 episodes P1\n",
      "0 earned after 55200 episodes P1\n",
      "0 earned after 55300 episodes P1\n",
      "1 earned after 55400 episodes P1\n",
      "1 earned after 55500 episodes P1\n",
      "1 earned after 55600 episodes P1\n",
      "0 earned after 55700 episodes P1\n",
      "1 earned after 55800 episodes P1\n",
      "1 earned after 55900 episodes P1\n",
      "1 earned after 56000 episodes P1\n",
      "1 earned after 56100 episodes P1\n",
      "1 earned after 56200 episodes P1\n",
      "1 earned after 56300 episodes P1\n",
      "1 earned after 56400 episodes P1\n",
      "1 earned after 56500 episodes P1\n",
      "1 earned after 56600 episodes P1\n",
      "1 earned after 56700 episodes P1\n",
      "1 earned after 56800 episodes P1\n",
      "1 earned after 56900 episodes P1\n",
      "1 earned after 57000 episodes P1\n",
      "1 earned after 57100 episodes P1\n",
      "0 earned after 57200 episodes P1\n",
      "1 earned after 57300 episodes P1\n",
      "1 earned after 57400 episodes P1\n",
      "1 earned after 57500 episodes P1\n",
      "1 earned after 57600 episodes P1\n",
      "1 earned after 57700 episodes P1\n",
      "1 earned after 57800 episodes P1\n",
      "1 earned after 57900 episodes P1\n",
      "0 earned after 58000 episodes P1\n",
      "1 earned after 58100 episodes P1\n",
      "1 earned after 58200 episodes P1\n",
      "1 earned after 58300 episodes P1\n",
      "1 earned after 58400 episodes P1\n",
      "1 earned after 58500 episodes P1\n",
      "1 earned after 58600 episodes P1\n",
      "1 earned after 58700 episodes P1\n",
      "1 earned after 58800 episodes P1\n",
      "1 earned after 58900 episodes P1\n",
      "1 earned after 59000 episodes P1\n",
      "1 earned after 59100 episodes P1\n",
      "1 earned after 59200 episodes P1\n",
      "1 earned after 59300 episodes P1\n",
      "1 earned after 59400 episodes P1\n",
      "1 earned after 59500 episodes P1\n",
      "1 earned after 59600 episodes P1\n",
      "1 earned after 59700 episodes P1\n",
      "1 earned after 59800 episodes P1\n",
      "1 earned after 59900 episodes P1\n",
      "1 earned after 60000 episodes P1\n",
      "1 earned after 60100 episodes P1\n",
      "1 earned after 60200 episodes P1\n",
      "1 earned after 60300 episodes P1\n",
      "1 earned after 60400 episodes P1\n",
      "1 earned after 60500 episodes P1\n",
      "1 earned after 60600 episodes P1\n",
      "1 earned after 60700 episodes P1\n",
      "1 earned after 60800 episodes P1\n",
      "1 earned after 60900 episodes P1\n",
      "0 earned after 61000 episodes P1\n",
      "1 earned after 61100 episodes P1\n",
      "1 earned after 61200 episodes P1\n",
      "1 earned after 61300 episodes P1\n",
      "1 earned after 61400 episodes P1\n",
      "1 earned after 61500 episodes P1\n",
      "1 earned after 61600 episodes P1\n",
      "1 earned after 61700 episodes P1\n",
      "1 earned after 61800 episodes P1\n",
      "1 earned after 61900 episodes P1\n",
      "1 earned after 62000 episodes P1\n",
      "1 earned after 62100 episodes P1\n",
      "1 earned after 62200 episodes P1\n",
      "1 earned after 62300 episodes P1\n",
      "1 earned after 62400 episodes P1\n",
      "0 earned after 62500 episodes P1\n",
      "1 earned after 62600 episodes P1\n",
      "1 earned after 62700 episodes P1\n",
      "1 earned after 62800 episodes P1\n",
      "0 earned after 62900 episodes P1\n",
      "1 earned after 63000 episodes P1\n",
      "1 earned after 63100 episodes P1\n",
      "1 earned after 63200 episodes P1\n",
      "1 earned after 63300 episodes P1\n",
      "1 earned after 63400 episodes P1\n",
      "1 earned after 63500 episodes P1\n",
      "1 earned after 63600 episodes P1\n",
      "1 earned after 63700 episodes P1\n",
      "1 earned after 63800 episodes P1\n",
      "1 earned after 63900 episodes P1\n",
      "1 earned after 64000 episodes P1\n",
      "1 earned after 64100 episodes P1\n",
      "1 earned after 64200 episodes P1\n",
      "1 earned after 64300 episodes P1\n",
      "1 earned after 64400 episodes P1\n",
      "1 earned after 64500 episodes P1\n",
      "1 earned after 64600 episodes P1\n",
      "1 earned after 64700 episodes P1\n",
      "1 earned after 64800 episodes P1\n",
      "1 earned after 64900 episodes P1\n",
      "1 earned after 65000 episodes P1\n",
      "1 earned after 65100 episodes P1\n",
      "1 earned after 65200 episodes P1\n",
      "1 earned after 65300 episodes P1\n",
      "1 earned after 65400 episodes P1\n",
      "1 earned after 65500 episodes P1\n",
      "1 earned after 65600 episodes P1\n",
      "1 earned after 65700 episodes P1\n",
      "1 earned after 65800 episodes P1\n",
      "1 earned after 65900 episodes P1\n",
      "1 earned after 66000 episodes P1\n",
      "1 earned after 66100 episodes P1\n",
      "1 earned after 66200 episodes P1\n",
      "1 earned after 66300 episodes P1\n",
      "1 earned after 66400 episodes P1\n",
      "1 earned after 66500 episodes P1\n",
      "1 earned after 66600 episodes P1\n",
      "1 earned after 66700 episodes P1\n",
      "1 earned after 66800 episodes P1\n",
      "1 earned after 66900 episodes P1\n",
      "1 earned after 67000 episodes P1\n",
      "1 earned after 67100 episodes P1\n",
      "1 earned after 67200 episodes P1\n",
      "1 earned after 67300 episodes P1\n",
      "1 earned after 67400 episodes P1\n",
      "1 earned after 67500 episodes P1\n",
      "1 earned after 67600 episodes P1\n",
      "1 earned after 67700 episodes P1\n",
      "1 earned after 67800 episodes P1\n",
      "1 earned after 67900 episodes P1\n",
      "1 earned after 68000 episodes P1\n",
      "1 earned after 68100 episodes P1\n",
      "1 earned after 68200 episodes P1\n",
      "1 earned after 68300 episodes P1\n",
      "1 earned after 68400 episodes P1\n",
      "1 earned after 68500 episodes P1\n",
      "1 earned after 68600 episodes P1\n",
      "1 earned after 68700 episodes P1\n",
      "1 earned after 68800 episodes P1\n",
      "1 earned after 68900 episodes P1\n",
      "0 earned after 69000 episodes P1\n",
      "1 earned after 69100 episodes P1\n",
      "0 earned after 69200 episodes P1\n",
      "0 earned after 69300 episodes P1\n",
      "1 earned after 69400 episodes P1\n",
      "1 earned after 69500 episodes P1\n",
      "1 earned after 69600 episodes P1\n",
      "1 earned after 69700 episodes P1\n",
      "1 earned after 69800 episodes P1\n",
      "1 earned after 69900 episodes P1\n",
      "1 earned after 70000 episodes P1\n",
      "0 earned after 70100 episodes P1\n",
      "1 earned after 70200 episodes P1\n",
      "1 earned after 70300 episodes P1\n",
      "1 earned after 70400 episodes P1\n",
      "1 earned after 70500 episodes P1\n",
      "1 earned after 70600 episodes P1\n",
      "0 earned after 70700 episodes P1\n",
      "1 earned after 70800 episodes P1\n",
      "1 earned after 70900 episodes P1\n",
      "1 earned after 71000 episodes P1\n",
      "0 earned after 71100 episodes P1\n",
      "0 earned after 71200 episodes P1\n",
      "1 earned after 71300 episodes P1\n",
      "1 earned after 71400 episodes P1\n",
      "0 earned after 71500 episodes P1\n",
      "0 earned after 71600 episodes P1\n",
      "1 earned after 71700 episodes P1\n",
      "1 earned after 71800 episodes P1\n",
      "1 earned after 71900 episodes P1\n",
      "0 earned after 72000 episodes P1\n",
      "0 earned after 72100 episodes P1\n",
      "1 earned after 72200 episodes P1\n",
      "1 earned after 72300 episodes P1\n",
      "1 earned after 72400 episodes P1\n",
      "1 earned after 72500 episodes P1\n",
      "1 earned after 72600 episodes P1\n",
      "1 earned after 72700 episodes P1\n",
      "1 earned after 72800 episodes P1\n",
      "1 earned after 72900 episodes P1\n",
      "1 earned after 73000 episodes P1\n",
      "1 earned after 73100 episodes P1\n",
      "1 earned after 73200 episodes P1\n",
      "1 earned after 73300 episodes P1\n",
      "1 earned after 73400 episodes P1\n",
      "1 earned after 73500 episodes P1\n",
      "1 earned after 73600 episodes P1\n",
      "1 earned after 73700 episodes P1\n",
      "1 earned after 73800 episodes P1\n",
      "1 earned after 73900 episodes P1\n",
      "1 earned after 74000 episodes P1\n",
      "1 earned after 74100 episodes P1\n",
      "1 earned after 74200 episodes P1\n",
      "1 earned after 74300 episodes P1\n",
      "0 earned after 74400 episodes P1\n",
      "1 earned after 74500 episodes P1\n",
      "1 earned after 74600 episodes P1\n",
      "1 earned after 74700 episodes P1\n",
      "1 earned after 74800 episodes P1\n",
      "1 earned after 74900 episodes P1\n",
      "1 earned after 75000 episodes P1\n",
      "1 earned after 75100 episodes P1\n",
      "1 earned after 75200 episodes P1\n",
      "1 earned after 75300 episodes P1\n",
      "1 earned after 75400 episodes P1\n",
      "0 earned after 75500 episodes P1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 earned after 75600 episodes P1\n",
      "0 earned after 75700 episodes P1\n",
      "1 earned after 75800 episodes P1\n",
      "1 earned after 75900 episodes P1\n",
      "1 earned after 76000 episodes P1\n",
      "1 earned after 76100 episodes P1\n",
      "1 earned after 76200 episodes P1\n",
      "1 earned after 76300 episodes P1\n",
      "0 earned after 76400 episodes P1\n",
      "1 earned after 76500 episodes P1\n",
      "1 earned after 76600 episodes P1\n",
      "1 earned after 76700 episodes P1\n",
      "1 earned after 76800 episodes P1\n",
      "1 earned after 76900 episodes P1\n",
      "1 earned after 77000 episodes P1\n",
      "1 earned after 77100 episodes P1\n",
      "1 earned after 77200 episodes P1\n",
      "1 earned after 77300 episodes P1\n",
      "1 earned after 77400 episodes P1\n",
      "0 earned after 77500 episodes P1\n",
      "1 earned after 77600 episodes P1\n",
      "1 earned after 77700 episodes P1\n",
      "1 earned after 77800 episodes P1\n",
      "1 earned after 77900 episodes P1\n",
      "1 earned after 78000 episodes P1\n",
      "1 earned after 78100 episodes P1\n",
      "1 earned after 78200 episodes P1\n",
      "1 earned after 78300 episodes P1\n",
      "1 earned after 78400 episodes P1\n",
      "1 earned after 78500 episodes P1\n",
      "1 earned after 78600 episodes P1\n",
      "1 earned after 78700 episodes P1\n",
      "1 earned after 78800 episodes P1\n",
      "1 earned after 78900 episodes P1\n",
      "1 earned after 79000 episodes P1\n",
      "1 earned after 79100 episodes P1\n",
      "1 earned after 79200 episodes P1\n",
      "1 earned after 79300 episodes P1\n",
      "1 earned after 79400 episodes P1\n",
      "1 earned after 79500 episodes P1\n",
      "1 earned after 79600 episodes P1\n",
      "1 earned after 79700 episodes P1\n",
      "1 earned after 79800 episodes P1\n",
      "1 earned after 79900 episodes P1\n",
      "1 earned after 80000 episodes P1\n",
      "1 earned after 80100 episodes P1\n",
      "0 earned after 80200 episodes P1\n",
      "1 earned after 80300 episodes P1\n",
      "1 earned after 80400 episodes P1\n",
      "1 earned after 80500 episodes P1\n",
      "1 earned after 80600 episodes P1\n",
      "1 earned after 80700 episodes P1\n",
      "1 earned after 80800 episodes P1\n",
      "1 earned after 80900 episodes P1\n",
      "1 earned after 81000 episodes P1\n",
      "0 earned after 81100 episodes P1\n",
      "1 earned after 81200 episodes P1\n",
      "0 earned after 81300 episodes P1\n",
      "1 earned after 81400 episodes P1\n",
      "1 earned after 81500 episodes P1\n",
      "1 earned after 81600 episodes P1\n",
      "1 earned after 81700 episodes P1\n",
      "1 earned after 81800 episodes P1\n",
      "1 earned after 81900 episodes P1\n",
      "1 earned after 82000 episodes P1\n",
      "1 earned after 82100 episodes P1\n",
      "1 earned after 82200 episodes P1\n",
      "1 earned after 82300 episodes P1\n",
      "1 earned after 82400 episodes P1\n",
      "1 earned after 82500 episodes P1\n",
      "1 earned after 82600 episodes P1\n",
      "1 earned after 82700 episodes P1\n",
      "1 earned after 82800 episodes P1\n",
      "1 earned after 82900 episodes P1\n",
      "0 earned after 83000 episodes P1\n",
      "1 earned after 83100 episodes P1\n",
      "0 earned after 83200 episodes P1\n",
      "1 earned after 83300 episodes P1\n",
      "1 earned after 83400 episodes P1\n",
      "1 earned after 83500 episodes P1\n",
      "1 earned after 83600 episodes P1\n",
      "1 earned after 83700 episodes P1\n",
      "1 earned after 83800 episodes P1\n",
      "1 earned after 83900 episodes P1\n",
      "1 earned after 84000 episodes P1\n",
      "1 earned after 84100 episodes P1\n",
      "1 earned after 84200 episodes P1\n",
      "1 earned after 84300 episodes P1\n",
      "0 earned after 84400 episodes P1\n",
      "1 earned after 84500 episodes P1\n",
      "1 earned after 84600 episodes P1\n",
      "1 earned after 84700 episodes P1\n",
      "1 earned after 84800 episodes P1\n",
      "1 earned after 84900 episodes P1\n",
      "1 earned after 85000 episodes P1\n",
      "1 earned after 85100 episodes P1\n",
      "1 earned after 85200 episodes P1\n",
      "1 earned after 85300 episodes P1\n",
      "1 earned after 85400 episodes P1\n",
      "1 earned after 85500 episodes P1\n",
      "1 earned after 85600 episodes P1\n",
      "1 earned after 85700 episodes P1\n",
      "1 earned after 85800 episodes P1\n",
      "1 earned after 85900 episodes P1\n",
      "1 earned after 86000 episodes P1\n",
      "1 earned after 86100 episodes P1\n",
      "1 earned after 86200 episodes P1\n",
      "1 earned after 86300 episodes P1\n",
      "1 earned after 86400 episodes P1\n",
      "1 earned after 86500 episodes P1\n",
      "1 earned after 86600 episodes P1\n",
      "0 earned after 86700 episodes P1\n",
      "1 earned after 86800 episodes P1\n",
      "1 earned after 86900 episodes P1\n",
      "1 earned after 87000 episodes P1\n",
      "1 earned after 87100 episodes P1\n",
      "1 earned after 87200 episodes P1\n",
      "1 earned after 87300 episodes P1\n",
      "1 earned after 87400 episodes P1\n",
      "1 earned after 87500 episodes P1\n",
      "1 earned after 87600 episodes P1\n",
      "1 earned after 87700 episodes P1\n",
      "1 earned after 87800 episodes P1\n",
      "1 earned after 87900 episodes P1\n",
      "1 earned after 88000 episodes P1\n",
      "0 earned after 88100 episodes P1\n",
      "1 earned after 88200 episodes P1\n",
      "1 earned after 88300 episodes P1\n",
      "1 earned after 88400 episodes P1\n",
      "1 earned after 88500 episodes P1\n",
      "1 earned after 88600 episodes P1\n",
      "1 earned after 88700 episodes P1\n",
      "1 earned after 88800 episodes P1\n",
      "1 earned after 88900 episodes P1\n",
      "1 earned after 89000 episodes P1\n",
      "1 earned after 89100 episodes P1\n",
      "1 earned after 89200 episodes P1\n",
      "0 earned after 89300 episodes P1\n",
      "1 earned after 89400 episodes P1\n",
      "1 earned after 89500 episodes P1\n",
      "1 earned after 89600 episodes P1\n",
      "1 earned after 89700 episodes P1\n",
      "1 earned after 89800 episodes P1\n",
      "1 earned after 89900 episodes P1\n",
      "1 earned after 90000 episodes P1\n",
      "1 earned after 90100 episodes P1\n",
      "1 earned after 90200 episodes P1\n",
      "0 earned after 90300 episodes P1\n",
      "0 earned after 90400 episodes P1\n",
      "1 earned after 90500 episodes P1\n",
      "1 earned after 90600 episodes P1\n",
      "1 earned after 90700 episodes P1\n",
      "0 earned after 90800 episodes P1\n",
      "1 earned after 90900 episodes P1\n",
      "1 earned after 91000 episodes P1\n",
      "0 earned after 91100 episodes P1\n",
      "1 earned after 91200 episodes P1\n",
      "1 earned after 91300 episodes P1\n",
      "1 earned after 91400 episodes P1\n",
      "1 earned after 91500 episodes P1\n",
      "1 earned after 91600 episodes P1\n",
      "1 earned after 91700 episodes P1\n",
      "0 earned after 91800 episodes P1\n",
      "0 earned after 91900 episodes P1\n",
      "1 earned after 92000 episodes P1\n",
      "1 earned after 92100 episodes P1\n",
      "1 earned after 92200 episodes P1\n",
      "1 earned after 92300 episodes P1\n",
      "1 earned after 92400 episodes P1\n",
      "0 earned after 92500 episodes P1\n",
      "1 earned after 92600 episodes P1\n",
      "1 earned after 92700 episodes P1\n",
      "1 earned after 92800 episodes P1\n",
      "1 earned after 92900 episodes P1\n",
      "1 earned after 93000 episodes P1\n",
      "1 earned after 93100 episodes P1\n",
      "1 earned after 93200 episodes P1\n",
      "1 earned after 93300 episodes P1\n",
      "1 earned after 93400 episodes P1\n",
      "0 earned after 93500 episodes P1\n",
      "0 earned after 93600 episodes P1\n",
      "1 earned after 93700 episodes P1\n",
      "0 earned after 93800 episodes P1\n",
      "1 earned after 93900 episodes P1\n",
      "1 earned after 94000 episodes P1\n",
      "0 earned after 94100 episodes P1\n",
      "1 earned after 94200 episodes P1\n",
      "1 earned after 94300 episodes P1\n",
      "1 earned after 94400 episodes P1\n",
      "1 earned after 94500 episodes P1\n",
      "1 earned after 94600 episodes P1\n",
      "1 earned after 94700 episodes P1\n",
      "1 earned after 94800 episodes P1\n",
      "1 earned after 94900 episodes P1\n",
      "1 earned after 95000 episodes P1\n",
      "1 earned after 95100 episodes P1\n",
      "1 earned after 95200 episodes P1\n",
      "1 earned after 95300 episodes P1\n",
      "1 earned after 95400 episodes P1\n",
      "1 earned after 95500 episodes P1\n",
      "1 earned after 95600 episodes P1\n",
      "1 earned after 95700 episodes P1\n",
      "1 earned after 95800 episodes P1\n",
      "1 earned after 95900 episodes P1\n",
      "1 earned after 96000 episodes P1\n",
      "1 earned after 96100 episodes P1\n",
      "1 earned after 96200 episodes P1\n",
      "1 earned after 96300 episodes P1\n",
      "1 earned after 96400 episodes P1\n",
      "1 earned after 96500 episodes P1\n",
      "1 earned after 96600 episodes P1\n",
      "1 earned after 96700 episodes P1\n",
      "1 earned after 96800 episodes P1\n",
      "1 earned after 96900 episodes P1\n",
      "1 earned after 97000 episodes P1\n",
      "1 earned after 97100 episodes P1\n",
      "1 earned after 97200 episodes P1\n",
      "0 earned after 97300 episodes P1\n",
      "1 earned after 97400 episodes P1\n",
      "1 earned after 97500 episodes P1\n",
      "1 earned after 97600 episodes P1\n",
      "1 earned after 97700 episodes P1\n",
      "1 earned after 97800 episodes P1\n",
      "1 earned after 97900 episodes P1\n",
      "1 earned after 98000 episodes P1\n",
      "1 earned after 98100 episodes P1\n",
      "1 earned after 98200 episodes P1\n",
      "1 earned after 98300 episodes P1\n",
      "1 earned after 98400 episodes P1\n",
      "1 earned after 98500 episodes P1\n",
      "1 earned after 98600 episodes P1\n",
      "1 earned after 98700 episodes P1\n",
      "1 earned after 98800 episodes P1\n",
      "1 earned after 98900 episodes P1\n",
      "1 earned after 99000 episodes P1\n",
      "1 earned after 99100 episodes P1\n",
      "1 earned after 99200 episodes P1\n",
      "1 earned after 99300 episodes P1\n",
      "1 earned after 99400 episodes P1\n",
      "1 earned after 99500 episodes P1\n",
      "1 earned after 99600 episodes P1\n",
      "1 earned after 99700 episodes P1\n",
      "1 earned after 99800 episodes P1\n",
      "1 earned after 99900 episodes P1\n"
     ]
    }
   ],
   "source": [
    "NUM_EP=0\n",
    "MAX_EP=100000\n",
    "\n",
    "while NUM_EP < MAX_EP:\n",
    "    cum_reward=0\n",
    "    while not (tc.env.episode_terminated):\n",
    "        current_state, action, next_state, reward = tc.run_episode(learning_phase=True)\n",
    "        cum_reward += reward\n",
    "    if NUM_EP % (int(MAX_EP/1000) if MAX_EP>1000 else 1) == 0:\n",
    "        print(\"{} earned after {} episodes P1\".format(cum_reward,NUM_EP))\n",
    "    NUM_EP += 1\n",
    "    tc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc.agent.policy.save_policy('tttpolicy_silver.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O' '_' '_']\n",
      " ['_' '_' '_']\n",
      " ['X' '_' '_']]\n",
      "[['O' 'O' '_']\n",
      " ['_' '_' '_']\n",
      " ['X' 'X' '_']]\n",
      "[['O' 'O' 'O']\n",
      " ['_' '_' '_']\n",
      " ['X' 'X' 'X']]\n"
     ]
    }
   ],
   "source": [
    "tc.reset()\n",
    "cum_reward = 0\n",
    "while not tc.env.episode_terminated:\n",
    "    current_state, action, next_state, reward = tc.run_episode(learning_phase=False)\n",
    "    print(np.reshape(next_state, (3,3)))\n",
    "    cum_reward += reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_'),\n",
       " ('_', 'X'),\n",
       " ('_', 'O'),\n",
       " ('X', '_'),\n",
       " ('X', 'X'),\n",
       " ('X', 'O'),\n",
       " ('O', '_'),\n",
       " ('O', 'X'),\n",
       " ('O', 'O')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
